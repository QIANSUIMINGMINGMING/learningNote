# 模型训练通用术语

#### batch:

字面上是批量的意思，在深度学习中指的是计算一次cost需要的输入数据个数。

主要的思想就是每次训练使用batch_size个数据进行参数寻优，一批中的数据共同决定了本次梯度的方向。

遍历全部数据的Batch gradient descent,计算量大,吃内存

没看一个数据就更新一次损失函数的 Stochastic随机gradient descent,两次迭代计算梯度差距很大,甚至方向相反,容易导致损失函数不[收敛](../basicMaths/函数收敛.md)

上述两者折中mini-batch gradient decent将数据集分成一定数量的批，梯度不容易跑偏容易收敛，同时减少了一次处理的数据数量，因而计算量也小了很多，速度较快。

![img](https://img-blog.csdnimg.cn/20181213174206640.png)
batch可以认为上述cost计算公式中的m。

