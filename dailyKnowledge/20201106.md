# 20201106

引子：解决二分类问题的逻辑回归算法，最关键的步骤就是将线性模型输出的实数域映射到[0, 1]表示概率分布的有效实数空间。

![img](https://pic4.zhimg.com/80/v2-7c4e9e545f0cc76bb5202fec4202f873_720w.jpg)

二分类问题：单输出节点（输出：![[公式]](https://www.zhihu.com/equation?tex=P%28A%7Cx%29)），双输出节点（约束：![[公式]](https://www.zhihu.com/equation?tex=P%28A%7Cx%29+%2B+P%28%5Cbar%7BA%7D%7Cx%29+%3D+1)）

推广二分类问题->**n个输出节点的n分类问题**

**softmax：** 将神经网络各个输出节点的输出值范围映射到[0, 1]，并且约束各个输出节点的输出值的和为1的函数

soft | max ：与只选出一个最大值的hardmax形成对比；为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。

![[公式]](https://www.zhihu.com/equation?tex=Softmax%28z_%7Bi%7D%29%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D)

其中 ![[公式]](https://www.zhihu.com/equation?tex=z_%7Bi%7D) 为第i个节点的输出值，C为输出节点的个数，即分类的类别个数。通过Softmax函数就可以将多分类的输出值转换为范围在[0, 1]和为1的概率分布

引入e是为了扩大i偏移的影响力![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7Bz_%7Bi%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7B3%7D%7Bz_%7Bc%7D%7D%7D)，且求导比较方便。缺点是当![[公式]](https://www.zhihu.com/equation?tex=z_%7Bi%7D) 特别大时数值溢出

解决：

![[公式]](https://www.zhihu.com/equation?tex=D+%3D+max%28z%29+)
![[公式]](https://www.zhihu.com/equation?tex=softmax%28z_%7Bi%7D%29%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D+-+D%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D-D%7D%7D%7D)

------------------------------------

通常与softmax同时使用的交叉熵损失函数

![img](https://upload-images.jianshu.io/upload_images/4264437-43557f411482456d.png?imageMogr2/auto-orient/strip|imageView2/2/w/391/format/webp)

解决二分类的sigmoid函数特点：其中 s 是模型上一层的输出，s = 0 时，g(s) = 0.5；s >> 0 时， g ≈ 1，s << 0 时，g ≈ 0。为啥是它：符合要求，且Sigmoid函数和正态分布函数的积分形式形状非常类似。正态分布很好，很常见，先猜上一手。

![img](https://upload-images.jianshu.io/upload_images/4264437-ce7d54cd3f34c9c7.png?imageMogr2/auto-orient/strip|imageView2/2/w/184/format/webp)

[极大似然估计](../basicMaths/MaximumLikelihoodEstimate.md)

