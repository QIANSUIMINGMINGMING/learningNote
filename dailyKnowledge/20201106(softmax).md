# 20201106

引子：解决二分类问题的逻辑回归算法，最关键的步骤就是将线性模型输出的实数域映射到[0, 1]表示概率分布的有效实数空间。

![img](https://pic4.zhimg.com/80/v2-7c4e9e545f0cc76bb5202fec4202f873_720w.jpg)

二分类问题：单输出节点（输出：![[公式]](https://www.zhihu.com/equation?tex=P%28A%7Cx%29)），双输出节点（约束：![[公式]](https://www.zhihu.com/equation?tex=P%28A%7Cx%29+%2B+P%28%5Cbar%7BA%7D%7Cx%29+%3D+1)）

推广二分类问题->**n个输出节点的n分类问题**

**softmax：** 将神经网络各个输出节点的输出值范围映射到[0, 1]，并且约束各个输出节点的输出值的和为1的函数

soft | max ：与只选出一个最大值的hardmax形成对比；为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。

![[公式]](https://www.zhihu.com/equation?tex=Softmax%28z_%7Bi%7D%29%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D)

其中 ![[公式]](https://www.zhihu.com/equation?tex=z_%7Bi%7D) 为第i个节点的输出值，C为输出节点的个数，即分类的类别个数。通过Softmax函数就可以将多分类的输出值转换为范围在[0, 1]和为1的概率分布

引入e是为了扩大i偏移的影响力![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7Bz_%7Bi%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7B3%7D%7Bz_%7Bc%7D%7D%7D)，且求导比较方便。缺点是当![[公式]](https://www.zhihu.com/equation?tex=z_%7Bi%7D) 特别大时数值溢出

解决：

![[公式]](https://www.zhihu.com/equation?tex=D+%3D+max%28z%29+)
![[公式]](https://www.zhihu.com/equation?tex=softmax%28z_%7Bi%7D%29%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D+-+D%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D-D%7D%7D%7D)

--------------------------------

#### 学习过程

交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数（检验模型与真实情况的不一致程度），此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和**sigmoid(或softmax)函数**一起出现。

我们用神经网络最后一层输出的情况，来看一眼整个模型预测、获得损失和学习的流程：

1. 神经网络最后一层得到每个类别的得分**scores**；
2. 该得分经过**sigmoid(或softmax)函数**获得概率输出；
3. 模型预测的类别概率输出与真实类别的one hot（分类特征数字化的一种编码方式）形式进行交叉熵损失函数的计算。

```
One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。
分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。
```

[损失函数](../basicMaths/)

![img](https://upload-images.jianshu.io/upload_images/4264437-43557f411482456d.png?imageMogr2/auto-orient/strip|imageView2/2/w/391/format/webp)

解决二分类的sigmoid函数特点：其中 s 是模型上一层的输出，s = 0 时，g(s) = 0.5；s >> 0 时， g ≈ 1，s << 0 时，g ≈ 0。为啥是它：符合要求，且Sigmoid函数和正态分布函数的积分形式形状非常类似。正态分布很好，很常见，先猜上一手。

![img](https://upload-images.jianshu.io/upload_images/4264437-ce7d54cd3f34c9c7.png?imageMogr2/auto-orient/strip|imageView2/2/w/184/format/webp)

[极大似然估计](../basicMaths/MLEandOthers.md)

在解决二分类问题时，把取0，1对应的分数映射到[0,1]区间得到取对应类别的概率![img](https://upload-images.jianshu.io/upload_images/4264437-b271df77a73a2abb.png?imageMogr2/auto-orient/strip|imageView2/2/w/178/format/webp)               ![img](https://upload-images.jianshu.io/upload_images/4264437-b79796d46af30c21.png?imageMogr2/auto-orient/strip|imageView2/2/w/223/format/webp)

对应的似然函数：![img](https://upload-images.jianshu.io/upload_images/4264437-20c0486d945ccfbf.png?imageMogr2/auto-orient/strip|imageView2/2/w/274/format/webp)

左右两边取对数（不会影响单调性）得到：![img](https://upload-images.jianshu.io/upload_images/4264437-31a9a4d168e5bba2.png?imageMogr2/auto-orient/strip|imageView2/2/w/689/format/webp)

训练模型时，目标是让模型尽量准确的选择到正类，即![img](https://upload-images.jianshu.io/upload_images/4264437-b271df77a73a2abb.png?imageMogr2/auto-orient/strip|imageView2/2/w/178/format/webp)尽量等于1

回顾损失函数的定义（检验模型与真实情况的不一致程度），模型越精确，损失函数应该越小，观察二分类问题下![img](https://upload-images.jianshu.io/upload_images/4264437-31a9a4d168e5bba2.png?imageMogr2/auto-orient/strip|imageView2/2/w/689/format/webp)取反后（y=1）的函数图像：

![img](https://upload-images.jianshu.io/upload_images/4264437-f10fafc8f887ee1c.png?imageMogr2/auto-orient/strip|imageView2/2/w/399/format/webp)

所以：![img](https://upload-images.jianshu.io/upload_images/4264437-e9a1204d56d80089.png?imageMogr2/auto-orient/strip|imageView2/2/w/384/format/webp)恰好符合对二分类问题损失函数的要求，另外，从图形中我们可以发现：预测输出与 y 差得越多，L 的值越大，也就是说对当前模型的 “ 惩罚 ” 越大，而且是非线性增大，是一种类似指数增长的级别。这是由 log 函数本身的特性所决定的。这样的好处是模型会倾向于让预测输出更接近真实样本标签 y。

多样本的情况：![img](https://upload-images.jianshu.io/upload_images/4264437-c2d2b6130519f5a5.png?imageMogr2/auto-orient/strip|imageView2/2/w/475/format/webp)